{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73660645f714cfd3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create pipeline for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24171ca9eff0e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:33:56.086197Z",
     "start_time": "2024-03-20T11:33:55.994665Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "X_train = pd.read_csv('../data/orig_train.csv')\n",
    "target = X_train['DEFAULT_FLAG']\n",
    "X_train.drop(columns=['DEFAULT_FLAG'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b2abdf8fb6a1a0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:45:05.186999Z",
     "start_time": "2024-03-20T11:45:05.174206Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Function to create dictionary containing distribution of a categorical column\n",
    "def get_col_distribution(X, col_name):\n",
    "    value_counts = X[col_name].value_counts()\n",
    "    number_of_missing_values = value_counts[\"Missing\"]\n",
    "    value_counts_dict = value_counts[1:].to_dict()\n",
    "    \n",
    "    # change to probabilities\n",
    "    for key in value_counts_dict:\n",
    "        value_counts_dict[key] = value_counts_dict[key] / (len(X) - number_of_missing_values)\n",
    "    \n",
    "    return value_counts_dict\n",
    "\n",
    "\n",
    "# Simple column remover to remove low variance columns\n",
    "class FeatureRemover(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_drop):\n",
    "        self.columns_to_drop = columns_to_drop\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(self.columns_to_drop,axis=1)\n",
    "\n",
    "# Update name of the column\n",
    "class FeatureNameUpdater(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, old_name, new_name):\n",
    "        self.old_name = old_name\n",
    "        self.new_name = new_name\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.rename(columns = {self.old_name: self.new_name}, inplace = True)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "# Imputes missing values with mode and according to distribution of columns\n",
    "class MyImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols_mode_imputation, cols_dist_imputation):\n",
    "        self.modes = {}\n",
    "        self.distributions_of_columns = {}\n",
    "        self.cols_mode_imputation = cols_mode_imputation\n",
    "        self.cols_dist_imputation = cols_dist_imputation\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        for col_name in self.cols_mode_imputation:\n",
    "            self.modes[col_name] = X[col_name].mode()[0]\n",
    "        \n",
    "        for col_name in self.cols_dist_imputation:\n",
    "            self.distributions_of_columns[col_name] = get_col_distribution(X, col_name)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        for col_name in self.cols_mode_imputation:\n",
    "            X_copy[col_name] = X[col_name].replace('Missing', self.modes[col_name])\n",
    "        \n",
    "        for col_name in self.cols_dist_imputation:\n",
    "            column_distribution = self.distributions_of_columns[col_name]\n",
    "            \n",
    "            X_copy[col_name] = X[col_name].replace('Missing',\n",
    "                                              np.random.choice(list(column_distribution.keys()), \n",
    "                                                        p = list(column_distribution.values())))\n",
    "        \n",
    "        return X_copy\n",
    "\n",
    "# Encoder for categorical variables handling both ordered and unordered ones\n",
    "class MyEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_features_wo_order, cat_feature_with_order, categories_order):\n",
    "        self.cat_features_wo_order = cat_features_wo_order\n",
    "        self.cat_feature_with_order = cat_feature_with_order\n",
    "        self.categories_order = categories_order\n",
    "        self.ordinal_encoder = OrdinalEncoder(categories=[categories_order])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.ordinal_encoder.fit(X[[self.cat_feature_with_order]])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_copy = pd.get_dummies(X, columns=self.cat_features_wo_order)\n",
    "\n",
    "        X_copy[self.cat_feature_with_order] = self.ordinal_encoder.transform(X[[self.cat_feature_with_order]])\n",
    "\n",
    "        return X_copy\n",
    "\n",
    "# scaling the data, based on choice it will either standarize(default) or normalize,\n",
    "class MyScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, standarize=True):\n",
    "        self.standarize = standarize\n",
    "        \n",
    "        if self.standarize is True:\n",
    "            self.scaler = StandardScaler()\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # we want to scale only numerical columns\n",
    "        X_num_cols = X.select_dtypes(include=['float64', 'int64'])\n",
    "        \n",
    "        self.scaler.fit(X_num_cols)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_num_cols = X.select_dtypes(include=['float64', 'int64'])\n",
    "        \n",
    "        # transform numerical columns\n",
    "        X_num_cols_transformed = self.scaler.transform(X_num_cols)\n",
    "        \n",
    "        # change to df to access columns\n",
    "        X_num_cols_transformed_df = pd.DataFrame(X_num_cols_transformed, columns=X_num_cols.columns, index=X_num_cols.index)\n",
    "\n",
    "        # change them in X\n",
    "        X_copy = X.copy()\n",
    "        for col_name in X_num_cols_transformed_df.columns:\n",
    "            X_copy[col_name] = X_num_cols_transformed_df[col_name]\n",
    "\n",
    "        return X_copy\n",
    "    \n",
    "\n",
    "class OutlierReplacer(BaseEstimator, TransformerMixin):\n",
    "    # It will replace outliers from columns based on provided dictionary(for now list of columns)\n",
    "    # If to_remove_dict[col_name] is true outliers will be removed (and exchanged with a proper quantile)\n",
    "    # It is possible to adjust parameter k, by default its set to 1.5 as usually it is done\n",
    "    def __init__(self, columns=None, k=1.5):\n",
    "        self.columns = columns\n",
    "        self.k = k\n",
    "        self.lower_bounds = {}\n",
    "        self.upper_bounds = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Compute lower and upper bounds for each specified column\n",
    "        if self.columns is None:\n",
    "            self.columns = X.columns\n",
    "        for col in self.columns:\n",
    "            q1 = X[col].quantile(0.25)\n",
    "            q3 = X[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - self.k * iqr\n",
    "            upper_bound = q3 + self.k * iqr\n",
    "            self.lower_bounds[col] = lower_bound\n",
    "            self.upper_bounds[col] = upper_bound\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Replace outliers in specified columns with calculated bounds\n",
    "        X_copy = X.copy()\n",
    "        for col in self.columns:\n",
    "            lower_bound = self.lower_bounds[col]\n",
    "            upper_bound = self.upper_bounds[col]\n",
    "            X_copy[col] = X_copy[col].clip(lower_bound, upper_bound)\n",
    "        return X_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48ae2dc8309fec06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T11:41:33.909172Z",
     "start_time": "2024-03-20T11:41:33.874520Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline_processing = Pipeline([\n",
    "    # first we remove low variance columns (chosen during EDA) and FINALIZED_LOAN as variable from future\n",
    "    ('feature_remover', FeatureRemover(['SAVING_ACCOUNT', 'FOREIGN_ACCOUNT', 'DEPOSIT', 'PENSION_FUNDS', 'FINALIZED_LOAN'])), # first 4 columns are removed because of low variance, FINALIZED_LOAN removed as a feature from future\n",
    "   \n",
    "    # then we update the name of column BUSINESS AGE to keep it consisten with the rest of columns\n",
    "    ('name_updater', FeatureNameUpdater('BUSINESS AGE', 'BUSINESS_AGE')),\n",
    "    \n",
    "    # we impute missing values with mode or according to distribution (in case when missing value is mode)\n",
    "    ('imputer', MyImputer(['AREA', 'EDUCATION'], ['ECONOMIC_SECTOR', 'EMPLOYEE_NO'])),\n",
    "    \n",
    "    # encoding columns (those in a list) with one-hot encoding as they have no order\n",
    "    # EMPLOYEE_NO is encoded using OrdinalEncoder with specified order of values\n",
    "    ('encoder', MyEncoder([\"PRODUCT\", \"AREA\", \"RESIDENTIAL_PLACE\", \"EDUCATION\", \"MARITAL_STATUS\",\n",
    "                           \"ECONOMIC_SECTOR\"], 'EMPLOYEE_NO', [\"between 0-10\", \"between 11-20\",\n",
    "                                                               \"between 21-50\", \"between 51-100\",\n",
    "                                                               \"between 101-250\", \"between 251-500\",\n",
    "                                                               \"between 501-1.000\", \"> 1.000\"])),\n",
    "    \n",
    "    # Replace outliers in chosen columns, you can also provide parameter k for the iqr scaling (1.5 default)\n",
    "    ('outlier_replacer', OutlierReplacer(['LENGTH_RELATIONSHIP_WITH_CLIENT', 'WORK_SENIORITY', 'BUSINESS_AGE', 'INCOME'])),\n",
    "    \n",
    "    # scaling numerical data, you can choose whether to standarize(True) or minmax(False) (TODO: fix scaling 0-1 type features)\n",
    "    ('scaler', MyScaler(standarize=False))\n",
    "])\n",
    "\n",
    "X_train_transformed = pipeline_processing.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888967b",
   "metadata": {},
   "source": [
    "From our EDA we remember that some categorical columns had values that appeared very rarely, hence some of the columns after encoding might have near zero variance and should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5cb870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCT_D : 13 out of 15097\n",
      "RESIDENTIAL_PLACE_Rental : 28 out of 15097\n",
      "EDUCATION_Primary school : 28 out of 15097\n",
      "PRODUCT_A : 42 out of 15097\n",
      "ECONOMIC_SECTOR_Real estate activities : 79 out of 15097\n",
      "ECONOMIC_SECTOR_Electricity and gas : 152 out of 15097\n",
      "ECONOMIC_SECTOR_Accommodation and food service activities : 159 out of 15097\n",
      "EDUCATION_Middle school : 195 out of 15097\n",
      "ECONOMIC_SECTOR_Agriculture, hunting and forestry : 196 out of 15097\n",
      "ECONOMIC_SECTOR_Mining and quarrying : 205 out of 15097\n",
      "RESIDENTIAL_PLACE_Other : 246 out of 15097\n",
      "ECONOMIC_SECTOR_Financial and insurance activities : 274 out of 15097\n",
      "ECONOMIC_SECTOR_Water supply : 333 out of 15097\n",
      "ECONOMIC_SECTOR_Professional, scientific and technical activities : 393 out of 15097\n",
      "ECONOMIC_SECTOR_Education : 460 out of 15097\n",
      "ECONOMIC_SECTOR_Information and communication : 517 out of 15097\n",
      "EDUCATION_College : 562 out of 15097\n",
      "ECONOMIC_SECTOR_Human health and social work activities : 668 out of 15097\n",
      "ECONOMIC_SECTOR_Transportation and storage : 691 out of 15097\n",
      "MARITAL_STATUS_widow : 798 out of 15097\n",
      "EDUCATION_Other : 819 out of 15097\n",
      "ECONOMIC_SECTOR_Other : 840 out of 15097\n",
      "RESIDENTIAL_PLACE_Owner with mortgage : 861 out of 15097\n",
      "MARITAL_STATUS_divorced : 1045 out of 15097\n",
      "EDUCATION_Post-graduate : 1046 out of 15097\n",
      "EDUCATION_Vocational school : 1228 out of 15097\n",
      "ECONOMIC_SECTOR_Public administration and defence : 1257 out of 15097\n",
      "ECONOMIC_SECTOR_Wholesale and retail trade : 1413 out of 15097\n",
      "PRODUCT_E : 1522 out of 15097\n",
      "EDUCATION_Post secondary school : 1620 out of 15097\n",
      "ECONOMIC_SECTOR_Manufacturing : 3093 out of 15097\n",
      "AREA_Urban area : 3286 out of 15097\n",
      "PRODUCT_F : 3334 out of 15097\n",
      "PRODUCT_B : 3407 out of 15097\n",
      "EDUCATION_Highschool : 3451 out of 15097\n",
      "AREA_Rural area : 4192 out of 15097\n",
      "ECONOMIC_SECTOR_Construction : 4367 out of 15097\n",
      "MARITAL_STATUS_single : 5113 out of 15097\n",
      "RESIDENTIAL_PLACE_Living with family : 5505 out of 15097\n",
      "EDUCATION_University : 6148 out of 15097\n",
      "PRODUCT_C : 6779 out of 15097\n",
      "AREA_County capital : 7619 out of 15097\n",
      "MARITAL_STATUS_married : 8141 out of 15097\n",
      "RESIDENTIAL_PLACE_Owner without mortgage : 8457 out of 15097\n"
     ]
    }
   ],
   "source": [
    "encoded_columns = X_train_transformed.select_dtypes(include=['bool'])\n",
    "\n",
    "# dictionary for saving number of true values in particular columns\n",
    "dict_true_values = {}\n",
    "\n",
    "for col_name in encoded_columns.columns:\n",
    "    # calculate number of true values and add to dict\n",
    "    number_of_true_values = encoded_columns[col_name].sum()\n",
    "    dict_true_values[col_name] = number_of_true_values\n",
    "\n",
    "# sort keys by values\n",
    "keys_sorted = sorted(dict_true_values, key=dict_true_values.get)\n",
    "\n",
    "for key in keys_sorted:\n",
    "    print(key, ':', dict_true_values[key], 'out of', len(X_train_transformed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5d1cd",
   "metadata": {},
   "source": [
    "We can safely delete PRODUCT_D, RESIDENTIAL_PLACE_Rental, EDUCATION_Primary School and PRODUCT_A\n",
    "(not sure were to set the threshold for removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b106b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['PRODUCT_D', 'RESIDENTIAL_PLACE_Rental', 'EDUCATION_Primary school', 'PRODUCT_A']\n",
    "X_train_transformed = X_train_transformed.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# although we already removed columns we will add this step to pipeline\n",
    "# so that whole data processing is saved in it\n",
    "pipeline_processing.steps.append(['encoded_feature_remover', FeatureRemover(columns_to_drop=columns_to_drop)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956443d5",
   "metadata": {},
   "source": [
    "# High correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0229300c",
   "metadata": {},
   "source": [
    "From EDA heat map we know that there were some highly correlated features, let's display them now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3735600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlated Pairs:\n",
      "('NO_OF_DEPENDENTS', 'HOUSEHOLD_MEMBERS', 0.73)\n",
      "('CURRENT_ACCOUNT', 'DEBIT_CARD', 0.81)\n",
      "('RESIDENTIAL_PLACE_Owner without mortgage', 'RESIDENTIAL_PLACE_Living with family', -0.85)\n",
      "('MARITAL_STATUS_married', 'HOUSEHOLD_MEMBERS', 0.78)\n",
      "('MARITAL_STATUS_single', 'MARITAL_STATUS_married', -0.77)\n",
      "\n",
      "Spearman Correlated Pairs:\n",
      "('CURRENT_ACCOUNT', 'DEBIT_CARD', 0.81)\n",
      "('RESIDENTIAL_PLACE_Owner without mortgage', 'RESIDENTIAL_PLACE_Living with family', -0.85)\n",
      "('MARITAL_STATUS_married', 'HOUSEHOLD_MEMBERS', 0.88)\n",
      "('MARITAL_STATUS_single', 'HOUSEHOLD_MEMBERS', -0.71)\n",
      "('MARITAL_STATUS_single', 'MARITAL_STATUS_married', -0.77)\n"
     ]
    }
   ],
   "source": [
    "def find_correlated_pairs(X, k):\n",
    "    # Calculate Pearson correlation coefficients\n",
    "    pearson_corr = X.corr(method='pearson')\n",
    "\n",
    "    # Calculate Spearman correlation coefficients\n",
    "    spearman_corr = X.corr(method='spearman')\n",
    "\n",
    "    # Initialize lists to store correlated pairs\n",
    "    pearson_correlated_pairs = []\n",
    "    spearman_correlated_pairs = []\n",
    "    \n",
    "    # Initialize set for storing column names\n",
    "    cols = set()\n",
    "\n",
    "    # Loop through each pair of features\n",
    "    for i in range(len(X.columns)):\n",
    "        for j in range(len(X.columns)):\n",
    "            if i > j:\n",
    "                # Check Pearson correlation coefficient\n",
    "                if abs(pearson_corr.iloc[i, j]) > k:\n",
    "                    col1, col2 = pearson_corr.columns[i], pearson_corr.columns[j]\n",
    "                    pearson_correlated_pairs.append((col1, col2, round(pearson_corr.loc[col1, col2], 2)))\n",
    "                    cols.add(col1)\n",
    "                    cols.add(col2)\n",
    "                # Check Spearman correlation coefficient\n",
    "                if abs(spearman_corr.iloc[i, j]) > k:\n",
    "                    col1, col2 = spearman_corr.columns[i], pearson_corr.columns[j]\n",
    "                    spearman_correlated_pairs.append((col1, col2, round(spearman_corr.loc[col1, col2], 2)))\n",
    "                    cols.add(col1)\n",
    "                    cols.add(col2)\n",
    "\n",
    "    return pearson_correlated_pairs, spearman_correlated_pairs, cols\n",
    "\n",
    "\n",
    "pearson_pairs, spearman_pairs, cols = find_correlated_pairs(X_train_transformed, 0.7)\n",
    "print(\"Pearson Correlated Pairs:\")\n",
    "for pair in pearson_pairs:\n",
    "    print(pair)\n",
    "print(\"\\nSpearman Correlated Pairs:\")\n",
    "for pair in spearman_pairs:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae304cdd",
   "metadata": {},
   "source": [
    "## Lets check correlation of these features with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91c7ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations with target (sorted in descending order):\n",
      "MARITAL_STATUS_single: 0.16\n",
      "MARITAL_STATUS_married: -0.14\n",
      "RESIDENTIAL_PLACE_Living with family: 0.12\n",
      "HOUSEHOLD_MEMBERS: -0.1\n",
      "RESIDENTIAL_PLACE_Owner without mortgage: -0.09\n",
      "CURRENT_ACCOUNT: -0.03\n",
      "DEBIT_CARD: -0.02\n",
      "NO_OF_DEPENDENTS: 0.01\n"
     ]
    }
   ],
   "source": [
    "def get_sorted_correlations(X, cols, target):\n",
    "    # Calculate correlations with target for each column in cols\n",
    "    correlations = {}\n",
    "    for col in cols:\n",
    "        correlation = target.corr(X[col])\n",
    "        correlations[col] = round(correlation, 2)\n",
    "\n",
    "    # Sort correlations in descending order\n",
    "    sorted_correlations = sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "    # Print sorted correlations\n",
    "    print(\"Correlations with target (sorted in descending order):\")\n",
    "    for col, correlation in sorted_correlations:\n",
    "        print(f\"{col}: {correlation}\")\n",
    "    \n",
    "    return sorted_correlations\n",
    "\n",
    "sorted_correlations = get_sorted_correlations(X_train_transformed, cols, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c70777",
   "metadata": {},
   "source": [
    "## Also lets see feature importance in RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15eb24c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Feature  Importance\n",
      "3                                              INCOME    0.155290\n",
      "0                                                 AGE    0.130838\n",
      "5                                        BUSINESS_AGE    0.087166\n",
      "4                                      WORK_SENIORITY    0.067820\n",
      "7                     LENGTH_RELATIONSHIP_WITH_CLIENT    0.056225\n",
      "6                                         EMPLOYEE_NO    0.054252\n",
      "9                                     CURRENT_ACCOUNT    0.019432\n",
      "2                                    NO_OF_DEPENDENTS    0.018308\n",
      "15                                AREA_County capital    0.018191\n",
      "16                                    AREA_Rural area    0.018139\n",
      "14                                          PRODUCT_F    0.017673\n",
      "1                                   HOUSEHOLD_MEMBERS    0.017530\n",
      "28                               EDUCATION_University    0.017318\n",
      "23                               EDUCATION_Highschool    0.017079\n",
      "17                                    AREA_Urban area    0.016630\n",
      "32                              MARITAL_STATUS_single    0.016409\n",
      "18               RESIDENTIAL_PLACE_Living with family    0.016366\n",
      "42                      ECONOMIC_SECTOR_Manufacturing    0.016180\n",
      "8                                          DEBIT_CARD    0.015464\n",
      "26                    EDUCATION_Post secondary school    0.015358\n",
      "12                                          PRODUCT_C    0.015326\n",
      "11                                          PRODUCT_B    0.014080\n",
      "50         ECONOMIC_SECTOR_Wholesale and retail trade    0.013478\n",
      "25                                    EDUCATION_Other    0.013405\n",
      "36                       ECONOMIC_SECTOR_Construction    0.011392\n",
      "21           RESIDENTIAL_PLACE_Owner without mortgage    0.011363\n",
      "31                             MARITAL_STATUS_married    0.010658\n",
      "10                                     SALARY_ACCOUNT    0.009793\n",
      "22                                  EDUCATION_College    0.009779\n",
      "29                        EDUCATION_Vocational school    0.009397\n",
      "44                              ECONOMIC_SECTOR_Other    0.008269\n",
      "48         ECONOMIC_SECTOR_Transportation and storage    0.008084\n",
      "24                            EDUCATION_Middle school    0.007447\n",
      "13                                          PRODUCT_E    0.006523\n",
      "40  ECONOMIC_SECTOR_Human health and social work a...    0.006129\n",
      "30                            MARITAL_STATUS_divorced    0.005974\n",
      "49                       ECONOMIC_SECTOR_Water supply    0.005676\n",
      "46  ECONOMIC_SECTOR_Public administration and defence    0.005633\n",
      "35  ECONOMIC_SECTOR_Agriculture, hunting and forestry    0.005588\n",
      "34  ECONOMIC_SECTOR_Accommodation and food service...    0.005237\n",
      "27                            EDUCATION_Post-graduate    0.004949\n",
      "33                               MARITAL_STATUS_widow    0.004200\n",
      "19                            RESIDENTIAL_PLACE_Other    0.004159\n",
      "45  ECONOMIC_SECTOR_Professional, scientific and t...    0.002557\n",
      "37                          ECONOMIC_SECTOR_Education    0.002023\n",
      "41      ECONOMIC_SECTOR_Information and communication    0.001762\n",
      "47             ECONOMIC_SECTOR_Real estate activities    0.001724\n",
      "20              RESIDENTIAL_PLACE_Owner with mortgage    0.001257\n",
      "39  ECONOMIC_SECTOR_Financial and insurance activi...    0.001125\n",
      "38                ECONOMIC_SECTOR_Electricity and gas    0.000940\n",
      "43               ECONOMIC_SECTOR_Mining and quarrying    0.000406\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train_transformed, target)\n",
    "\n",
    "# Get feature importances from the classifier\n",
    "feature_importances = rf_classifier.feature_importances_\n",
    "\n",
    "# Create a DataFrame and sort by importance\n",
    "importance_df = pd.DataFrame({'Feature': X_train_transformed.columns, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae47575",
   "metadata": {},
   "source": [
    "## Summarize these outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b0e85",
   "metadata": {},
   "source": [
    "This summarization below contains no new data, it was just created for pracitcal reason to avoid scrolling through cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55a1cee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NO_OF_DEPENDENTS', 'HOUSEHOLD_MEMBERS', 0.73)\n",
      "                            NO_OF_DEPENDENTS  HOUSEHOLD_MEMBERS\n",
      "0  Correlation with target          0.010000           -0.10000\n",
      "1       Feature importance          0.018308            0.01753 \n",
      "\n",
      "('CURRENT_ACCOUNT', 'DEBIT_CARD', 0.81)\n",
      "                            CURRENT_ACCOUNT  DEBIT_CARD\n",
      "0  Correlation with target        -0.030000   -0.020000\n",
      "1       Feature importance         0.019432    0.015464 \n",
      "\n",
      "('RESIDENTIAL_PLACE_Owner without mortgage', 'RESIDENTIAL_PLACE_Living with family', -0.85)\n",
      "                            RESIDENTIAL_PLACE_Owner without mortgage  \\\n",
      "0  Correlation with target                                 -0.090000   \n",
      "1       Feature importance                                  0.011363   \n",
      "\n",
      "   RESIDENTIAL_PLACE_Living with family  \n",
      "0                              0.120000  \n",
      "1                              0.016366   \n",
      "\n",
      "('MARITAL_STATUS_married', 'HOUSEHOLD_MEMBERS', 0.78)\n",
      "                            MARITAL_STATUS_married  HOUSEHOLD_MEMBERS\n",
      "0  Correlation with target               -0.140000           -0.10000\n",
      "1       Feature importance                0.010658            0.01753 \n",
      "\n",
      "('MARITAL_STATUS_single', 'MARITAL_STATUS_married', -0.77)\n",
      "                            MARITAL_STATUS_single  MARITAL_STATUS_married\n",
      "0  Correlation with target               0.160000               -0.140000\n",
      "1       Feature importance               0.016409                0.010658 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def create_table(X, triple, feature_importances):\n",
    "    # Extract elements from the triple\n",
    "    col1, col2, correlation = triple\n",
    "\n",
    "    # Get correlation with target for each column\n",
    "    corr_with_target_col1 = round(target.corr(X[col1]), 2)\n",
    "    corr_with_target_col2 = round(target.corr(X[col2]), 2)\n",
    "\n",
    "    # Get feature importances for each column\n",
    "    importance_col1 = feature_importances.loc[feature_importances['Feature'] == col1, 'Importance'].values[0]\n",
    "    importance_col2 = feature_importances.loc[feature_importances['Feature'] == col2, 'Importance'].values[0]\n",
    "\n",
    "    # Construct the table as a DataFrame\n",
    "    table = pd.DataFrame({\n",
    "        '': ['Correlation with target', 'Feature importance'],\n",
    "        col1: [corr_with_target_col1, importance_col1],\n",
    "        col2: [corr_with_target_col2, importance_col2]\n",
    "    })\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "\n",
    "for pair in pearson_pairs:\n",
    "    print(pair)\n",
    "    table = create_table(X_train_transformed, pair, importance_df)\n",
    "    print(table, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3478ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO add comments and decide what feature should be removed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
